{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99650,"databundleVersionId":11917221,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vanpatangan/interval-prediction-house-prices?scriptVersionId=248791163\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:11.603506Z","iopub.execute_input":"2025-07-04T10:43:11.60425Z","iopub.status.idle":"2025-07-04T10:43:12.661126Z","shell.execute_reply.started":"2025-07-04T10:43:11.604218Z","shell.execute_reply":"2025-07-04T10:43:12.660286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# load files","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/prediction-interval-competition-ii-house-price/dataset.csv')\ntest = pd.read_csv('/kaggle/input/prediction-interval-competition-ii-house-price/test.csv')\n#sample_sub = pd.read_csv('/kaggle/input/prediction-interval-competition-ii-house-price/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:12.662752Z","iopub.execute_input":"2025-07-04T10:43:12.66328Z","iopub.status.idle":"2025-07-04T10:43:14.538865Z","shell.execute_reply.started":"2025-07-04T10:43:12.66325Z","shell.execute_reply":"2025-07-04T10:43:14.538094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# explore data","metadata":{}},{"cell_type":"code","source":"def check(df):\n    \"\"\"\n    Generates a concise summary of DataFrame columns.\n    \"\"\"\n    # Use list comprehension to iterate over each column\n    summary = [\n        [col, df[col].dtype, df[col].count(), df[col].nunique(), df[col].isnull().sum(), df.duplicated().sum()]\n        for col in df.columns\n    ]\n\n    # Create a DataFrame from the list of lists\n    df_check = pd.DataFrame(summary, columns=[\"column\", \"dtype\", \"instances\", \"unique\", \"sum_null\", \"duplicates\"])\n\n    return df_check","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:14.539756Z","iopub.execute_input":"2025-07-04T10:43:14.539995Z","iopub.status.idle":"2025-07-04T10:43:14.545769Z","shell.execute_reply.started":"2025-07-04T10:43:14.539977Z","shell.execute_reply":"2025-07-04T10:43:14.544894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training Data Summary\")\ndisplay(check(train))\n#display(train.head())\n\nprint(\"Test Data Summary\")\ndisplay(check(test))\n#display(test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:14.547478Z","iopub.execute_input":"2025-07-04T10:43:14.547745Z","iopub.status.idle":"2025-07-04T10:43:35.237146Z","shell.execute_reply.started":"2025-07-04T10:43:14.547716Z","shell.execute_reply":"2025-07-04T10:43:35.236256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Numerical features \nnumerical_features = train.select_dtypes(include=['float64', 'int64']).columns\n\n# Determine number of rows and columns for the grid\nnum_features = len(numerical_features)\nnum_cols = 4\nnum_rows = (num_features + num_cols - 1) // num_cols\n\n# Set a custom color palette\ncolors = sns.color_palette(\"husl\", n_colors=num_features)\n\n# Create subplots with a smaller figure size to reduce rendering time\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 4*num_rows), facecolor='white')\naxes = axes.flatten()\n\n# Optimize histogram plotting\nfor i, feature in enumerate(numerical_features):\n    # Use a subset of data \n    data = train[feature].dropna()  # Drop NaNs upfront\n    sns.histplot(data=data, bins=30, ax=axes[i], kde=False, color=colors[i], \n                 edgecolor='black', linewidth=0.5, stat='count')\n    axes[i].set_title(f'{feature}', fontsize=12, pad=8)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='both', labelsize=8)\n\n# Remove empty subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\n# Minimize layout adjustments\nplt.suptitle('Histograms of Numerical Features', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:35.237971Z","iopub.execute_input":"2025-07-04T10:43:35.238232Z","iopub.status.idle":"2025-07-04T10:43:47.433385Z","shell.execute_reply.started":"2025-07-04T10:43:35.238209Z","shell.execute_reply":"2025-07-04T10:43:47.432475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# correlation matrix","metadata":{}},{"cell_type":"code","source":"# Specify numerical columns in train\nnumerical_cols = train.select_dtypes(include=['int64', 'float64']).columns\n\n# Calculate correlation matrix for numerical features only\ncorr_matrix = train[numerical_cols].corr()\n\n# Plot the correlation matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f')\nplt.title(\"Correlation Matrix for Numerical Features in train\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:47.434135Z","iopub.execute_input":"2025-07-04T10:43:47.434417Z","iopub.status.idle":"2025-07-04T10:43:49.056174Z","shell.execute_reply.started":"2025-07-04T10:43:47.434391Z","shell.execute_reply":"2025-07-04T10:43:49.055229Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# features and preprocessing","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom category_encoders import TargetEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# Define feature types\nnumeric_features = [\n    'sqft', 'sqft_lot', 'land_val', 'imp_val', 'year_built', 'year_reno', \n    'sqft_fbsmt', 'garb_sqft', 'gara_sqft', 'grade', 'fbsmt_grade', \n    'condition', 'stories', 'beds', 'bath_full', 'bath_3qtr', 'bath_half',\n    'latitude', 'longitude'\n]\ncategorical_features = ['city', 'zoning', 'join_status', 'sale_warning']\nhigh_cardinality_features = ['subdivision', 'submarket']\nbinary_features = [\n    'wfnt', 'golf', 'greenbelt', 'view_rainier', 'view_olympics', \n    'view_cascades', 'view_territorial', 'view_skyline', 'view_sound', \n    'view_lakewash', 'view_lakesamm', 'view_otherwater', 'view_other', \n    'noise_traffic'  # Added noise_traffic to binary features\n]\n\n# Feature engineering function\ndef feature_engineering(df):\n    df = df.copy()\n    \n    # Datetime features\n    df[\"sale_date\"] = pd.to_datetime(df[\"sale_date\"])\n    df[\"sale_year\"] = df[\"sale_date\"].dt.year\n    df[\"sale_month\"] = df[\"sale_date\"].dt.month\n    df[\"sale_day\"] = df[\"sale_date\"].dt.day\n    df[\"sale_dow\"] = df[\"sale_date\"].dt.weekday\n    df[\"sale_quarter\"] = df[\"sale_date\"].dt.quarter\n    df[\"season\"] = df[\"sale_month\"] % 12 // 3\n\n    # Building age and renovation\n    df[\"age_at_sale\"] = df[\"sale_year\"] - df[\"year_built\"]\n    df[\"reno_age_at_sale\"] = df[\"sale_year\"] - df[\"year_reno\"]\n    df[\"is_renovated\"] = (df[\"year_reno\"] > df[\"year_built\"]).astype(int)\n\n    # Size ratios with protection against division by zero\n    df[\"lot_sqft_ratio\"] = df[\"sqft\"] / (df[\"sqft_lot\"].clip(lower=1))\n    df[\"garage_total\"] = df[\"garb_sqft\"] + df[\"gara_sqft\"]\n    df[\"bath_total\"] = df[\"bath_full\"] + df[\"bath_3qtr\"] + 0.5 * df[\"bath_half\"]\n    df[\"sqft_per_bed\"] = df[\"sqft\"] / (df[\"beds\"].clip(lower=1))\n    df[\"sqft_per_bath\"] = df[\"sqft\"] / (df[\"bath_total\"].clip(lower=1))\n\n    # Log-transformed features\n    df[\"log_sqft\"] = np.log1p(df[\"sqft\"])\n    df[\"log_sqft_lot\"] = np.log1p(df[\"sqft_lot\"])\n\n    # Aggregate view features\n    view_cols = [\n        'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n        'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm', 'view_otherwater', 'view_other'\n    ]\n    df[\"total_views\"] = df[view_cols].sum(axis=1)\n\n    # Binary features \n    df['has_waterfront'] = df['wfnt'].clip(0, 1)\n    df['is_golf'] = df['golf'].clip(0, 1)\n    df['is_greenbelt'] = df['greenbelt'].clip(0, 1)\n    df['has_traffic_noise'] = df['noise_traffic'].clip(0, 1)\n    df['has_rainier_view'] = df['view_rainier'].clip(0, 1)\n    df['has_olympics_view'] = df['view_olympics'].clip(0, 1)\n    df['has_cascades_view'] = df['view_cascades'].clip(0, 1)\n    df['has_view'] = (df[\"total_views\"] > 0).astype(int)\n\n    return df\n\n# Apply feature engineering\ntrain_fe = feature_engineering(train)\ntest_fe = feature_engineering(test)\n\n# Update feature lists with new features\nnumeric_features += [\n    'sale_year', 'sale_month', 'sale_day', 'sale_dow', 'sale_quarter', 'season',\n    'age_at_sale', 'reno_age_at_sale', 'lot_sqft_ratio', 'garage_total',\n    'bath_total', 'sqft_per_bed', 'sqft_per_bath', 'log_sqft', 'log_sqft_lot', 'total_views'\n]\nbinary_features += [\n    'is_renovated', 'has_waterfront', 'is_golf', 'is_greenbelt', \n    'has_traffic_noise', 'has_rainier_view', 'has_olympics_view', \n    'has_cascades_view', 'has_view'\n]\n\n# Preprocessing pipeline\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\nhigh_cardinality_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n    ('target', TargetEncoder())\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features),\n        ('high_card', high_cardinality_transformer, high_cardinality_features),\n        ('binary', 'passthrough', binary_features)\n    ])\n\n# Apply preprocessing\ny_train = np.log1p(train_fe['sale_price'])  # Log transform target\nX_train = train_fe.drop(columns=['sale_price', 'id', 'sale_date', 'sale_nbr'])\nX_test = test_fe.drop(columns=['id', 'sale_date', 'sale_nbr'])\n\n# Fit and transform with TargetEncoder\nX_train_transformed = preprocessor.fit_transform(X_train, y=y_train)\nX_test_transformed = preprocessor.transform(X_test)\n\nprint(\"New Features and Encoding done! 🤖😌\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:49.057166Z","iopub.execute_input":"2025-07-04T10:43:49.057423Z","iopub.status.idle":"2025-07-04T10:43:58.195632Z","shell.execute_reply.started":"2025-07-04T10:43:49.057402Z","shell.execute_reply":"2025-07-04T10:43:58.194803Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# train model","metadata":{}},{"cell_type":"code","source":"# Train Validation Split\nX_train_split, X_val, y_train_split, y_val = train_test_split(X_train_transformed, y_train, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:58.196556Z","iopub.execute_input":"2025-07-04T10:43:58.197273Z","iopub.status.idle":"2025-07-04T10:43:58.598327Z","shell.execute_reply.started":"2025-07-04T10:43:58.19724Z","shell.execute_reply":"2025-07-04T10:43:58.597497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_pinball_loss\nimport optuna\nimport logging\n\n# Set Optuna to only show warnings or errors\noptuna.logging.set_verbosity(logging.WARNING)\n\n\n# Train Quantile LGBM\ndef train_lgb_quantile(X_train, y_train, X_val, y_val, alpha, params):\n    params = params.copy()\n    params.update({\n        \"objective\": \"quantile\",\n        \"alpha\": alpha,\n        \"metric\": \"quantile\",\n        \"verbosity\": -1,\n        \"random_state\": 42\n    })\n\n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dval = lgb.Dataset(X_val, label=y_val)\n\n    model = lgb.train(\n        params,\n        dtrain,\n        valid_sets=[dval],\n        num_boost_round=1000,\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50),\n            lgb.log_evaluation(period=0)  # No evaluation logs\n        ]\n    )\n    return model\n\n\n# Objective for Optuna\ndef objective(trial):\n    params = {\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 128),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n        \"bagging_freq\": 1,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.05),\n        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 10)\n    }\n\n    model = train_lgb_quantile(X_train_split, y_train_split, X_val, y_val, alpha=0.5, params=params)\n    preds = model.predict(X_val)\n    loss = mean_pinball_loss(y_val, preds, alpha=0.5)\n    return loss\n\n# Run Optuna\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=30)\n\nbest_params = study.best_params\n#print(\"Best Params:\", best_params)\n\n# Final model params\nbase_params = best_params.copy()\nbase_params.update({\n    \"bagging_freq\": 1,\n    \"random_state\": 42\n})\n\n# Train final models\nmodel_lower = train_lgb_quantile(X_train_split, y_train_split, X_val, y_val, alpha=0.05, params=base_params)\nmodel_median = train_lgb_quantile(X_train_split, y_train_split, X_val, y_val, alpha=0.5, params=base_params)\nmodel_upper = train_lgb_quantile(X_train_split, y_train_split, X_val, y_val, alpha=0.95, params=base_params)\n\n# Predict (log scale)\npred_lower = model_lower.predict(X_val)\npred_upper = model_upper.predict(X_val)\nactual_y = np.expm1(y_val)\npred_lower_exp = np.expm1(pred_lower)\npred_upper_exp = np.expm1(pred_upper)\nmedian_pred = np.expm1(model_median.predict(X_val))\n\n# Winkler Score Function\ndef winkler_score(y_true, y_lower, y_upper, alpha):\n    score = np.where(\n        (y_true >= y_lower) & (y_true <= y_upper),\n        y_upper - y_lower,\n        (y_upper - y_lower) + (2 / alpha) * np.where(y_true < y_lower, y_lower - y_true, y_true - y_upper)\n    )\n    return np.mean(score)\n\n# Calculate Winkler Score\nwinkler = winkler_score(actual_y, pred_lower_exp, pred_upper_exp, alpha=0.1)\ncoverage = ((actual_y >= pred_lower_exp) & (actual_y <= pred_upper_exp)).mean()\navg_width = np.mean(pred_upper_exp - pred_lower_exp)\n\nprint(f\"[Winkler] Coverage: {coverage:.2%}\")\nprint(f\"[Winkler] Interval Width: {avg_width:.2f}\")\nprint(f\"[Winkler] Interval Score: {winkler:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:43:58.599158Z","iopub.execute_input":"2025-07-04T10:43:58.599434Z","iopub.status.idle":"2025-07-04T10:57:41.889666Z","shell.execute_reply.started":"2025-07-04T10:43:58.599408Z","shell.execute_reply":"2025-07-04T10:57:41.88884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization\nplt.figure(figsize=(14, 6))\nplt.plot(actual_y.values[:100], label=\"Actual\", marker=\"o\")\nplt.plot(median_pred[:100], label=\"Median Prediction\", marker=\"x\")\nplt.fill_between(range(100), pred_lower_exp[:100], pred_upper_exp[:100], alpha=0.3, label=\"Prediction Interval\")\nplt.legend()\nplt.title(\"Prediction Intervals vs Actuals (First 100)\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Sale Price\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:57:41.891812Z","iopub.execute_input":"2025-07-04T10:57:41.892178Z","iopub.status.idle":"2025-07-04T10:57:42.143321Z","shell.execute_reply.started":"2025-07-04T10:57:41.892154Z","shell.execute_reply":"2025-07-04T10:57:42.142365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# submission","metadata":{}},{"cell_type":"code","source":"# predictions on the test set\n\npi_test_lower = np.expm1(model_lower.predict(X_test_transformed))  \npi_test_median = np.expm1(model_median.predict(X_test_transformed))  \npi_test_upper = np.expm1(model_upper.predict(X_test_transformed))  \n\n# Export predictions\nsubmission = pd.DataFrame({\n    'id': test_fe['id'],  \n    'pi_lower': pi_test_lower,\n    'pi_upper': pi_test_upper\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission = pd.read_csv(\"submission.csv\")\nsubmission.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:57:42.144319Z","iopub.execute_input":"2025-07-04T10:57:42.144584Z","iopub.status.idle":"2025-07-04T10:58:12.728232Z","shell.execute_reply.started":"2025-07-04T10:57:42.144564Z","shell.execute_reply":"2025-07-04T10:58:12.727288Z"}},"outputs":[],"execution_count":null}]}
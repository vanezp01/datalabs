{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8587,"databundleVersionId":868304,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vanpatangan/predict-future-sales?scriptVersionId=219783415\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# load data","metadata":{}},{"cell_type":"code","source":"sales_train = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\ntest_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# data summary","metadata":{}},{"cell_type":"code","source":"def check(df):\n    \"\"\"\n    Generates a concise summary of DataFrame columns.\n    \"\"\"\n    # Compute values that are constant across columns\n    total_rows = len(df)\n    duplicates = df.duplicated().sum()\n\n    # Use vectorized operations \n    dtypes = df.dtypes\n    instances = df.count()\n    unique = df.nunique()\n    sum_null = df.isnull().sum()\n\n    # Create the summary \n    df_check = pd.DataFrame({\n        'column': df.columns,\n        'dtype': dtypes,\n        'instances': instances,\n        'unique': unique,\n        'sum_null': sum_null,\n        'duplicates': duplicates  \n    })\n\n    return df_check\n\nprint(\"Sales Train\")\ndisplay(check(sales_train))\ndisplay(sales_train.head())\n\nprint(\"Items\")\ndisplay(check(items))\ndisplay(items.head())\n\nprint(\"Item Categories\")\ndisplay(check(item_categories))\ndisplay(item_categories.head())\n\nprint(\"Shops\")\ndisplay(check(shops))\ndisplay(shops.head())\n\nprint(\"Test\")\ndisplay(check(test_df))\ndisplay(test_df.head())","metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# merge","metadata":{}},{"cell_type":"code","source":"# Merge sales_train with items, item_categories, and shops\ntrain = sales_train.merge(items, on='item_id', how='left') \\\n                   .merge(item_categories, on='item_category_id', how='left') \\\n                   .merge(shops, on='shop_id', how='left')\n\n# Get train columns excluding the target variable\ntrain_columns = [col for col in train.columns if col != \"item_cnt_day\"]\n\n# Find missing columns in test_df\nmissing_cols = set(train_columns) - set(test_df.columns)\n\n# Add missing columns to test_df with None (which becomes NaN in pandas)\nfor col in missing_cols:\n    test_df[col] = None\n\n# Make sure column order matches train (excluding target variable)\ntest = test_df[train_columns]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert date column to datetime \ntrain['date'] = pd.to_datetime(train['date'], dayfirst=True)\ntest['date'] = pd.to_datetime(test['date'], dayfirst=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA visualization","metadata":{}},{"cell_type":"code","source":"# Distribution plots for item_price and item_cnt_day\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nsns.histplot(train['item_price'], bins=30, kde=True, ax=axes[0])\naxes[0].set_title('Distribution of Item Price')\naxes[0].set_xlabel('Item Price')\n\nsns.histplot(train['item_cnt_day'], bins=30, kde=True, ax=axes[1])\naxes[1].set_title('Distribution of Item Count per Day')\naxes[1].set_xlabel('Item Count per Day')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Boxplots for outlier detection\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nsns.boxplot(y=train['item_price'], ax=axes[0])\naxes[0].set_title('Boxplot of Item Price')\n\nsns.boxplot(y=train['item_cnt_day'], ax=axes[1])\naxes[1].set_title('Boxplot of Item Count per Day')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute correlation matrix\ncorrelation_matrix = train[['date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day']].corr()\n\n# Plot correlation heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Total sales per month\nmonthly_sales = train.groupby('date_block_num')['item_cnt_day'].sum().reset_index()\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=monthly_sales, x='date_block_num', y='item_cnt_day')\nplt.title('Total Sales Over Time (Monthly)')\nplt.xlabel('Date Block Number')\nplt.ylabel('Total Sales')\nplt.show()\n\n# Top 10 shops by total sales\ntop_shops = train.groupby('shop_name')['item_cnt_day'].sum().sort_values(ascending=False).head(10)\ntop_shops.plot(kind='bar', figsize=(10, 6), title='Top 10 Shops by Sales')\nplt.ylabel('Total Sales')\nplt.show()\n\n# Top 10 categories by total sales\ntop_categories = train.groupby('item_category_name')['item_cnt_day'].sum().sort_values(ascending=False).head(10)\ntop_categories.plot(kind='bar', figsize=(10, 6), title='Top 10 Categories by Sales')\nplt.ylabel('Total Sales')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# handle outliers","metadata":{}},{"cell_type":"code","source":"# Calculate IQR for item_price\nQ1_price = train['item_price'].quantile(0.25)\nQ3_price = train['item_price'].quantile(0.75)\nIQR_price = Q3_price - Q1_price\nprice_lower = Q1_price - 1.5 * IQR_price\nprice_upper = Q3_price + 1.5 * IQR_price\n\n# Calculate IQR for item_cnt_day\nQ1_cnt = train['item_cnt_day'].quantile(0.25)\nQ3_cnt = train['item_cnt_day'].quantile(0.75)\nIQR_cnt = Q3_cnt - Q1_cnt\ncnt_lower = Q1_cnt - 1.5 * IQR_cnt\ncnt_upper = Q3_cnt + 1.5 * IQR_cnt\n\n# Cap outliers for item_price\ntrain['item_price'] = train['item_price'].clip(lower=price_lower, upper=price_upper)\n\n# Cap outliers for item_cnt_day\ntrain['item_cnt_day'] = train['item_cnt_day'].clip(lower=cnt_lower, upper=cnt_upper)\n\n# Plot for item_price\nplt.figure(figsize=(10, 5))\nplt.hist(train['item_price'], bins=50, color='blue', alpha=0.7)\nplt.axvline(price_lower, color='red', linestyle='--', label='Lower Bound')\nplt.axvline(price_upper, color='green', linestyle='--', label='Upper Bound')\nplt.title('Distribution of item_price after capping')\nplt.legend()\nplt.show()\n\n# Plot for item_cnt_day\nplt.figure(figsize=(10, 5))\nplt.hist(train['item_cnt_day'], bins=50, color='orange', alpha=0.7)\nplt.axvline(cnt_lower, color='red', linestyle='--', label='Lower Bound')\nplt.axvline(cnt_upper, color='green', linestyle='--', label='Upper Bound')\nplt.title('Distribution of item_cnt_day after capping')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# feature engineering","metadata":{}},{"cell_type":"code","source":"def create_features(train, test):\n    # Ensure consistent data types\n    train['item_category_id'] = train['item_category_id'].fillna(-1).astype(int)\n    test['item_category_id'] = test['item_category_id'].fillna(-1).astype(int)\n\n    # Historical Aggregations\n    train['shop_avg_sales'] = train.groupby('shop_id')['item_cnt_day'].transform('mean')\n    train['category_avg_sales'] = train.groupby('item_category_id')['item_cnt_day'].transform('mean')\n\n    # Merge aggregated features into test using latest available values\n    shop_avg_sales = train.groupby('shop_id', as_index=False)['item_cnt_day'].mean().rename(columns={'item_cnt_day': 'shop_avg_sales'})\n    category_avg_sales = train.groupby('item_category_id', as_index=False)['item_cnt_day'].mean().rename(columns={'item_cnt_day': 'category_avg_sales'})\n\n    test = test.merge(shop_avg_sales, on='shop_id', how='left')\n    test = test.merge(category_avg_sales, on='item_category_id', how='left')\n\n    # Cumulative sum (test does not have sales history, so set to 0)\n    train['cumulative_sales'] = train.groupby(['shop_id', 'item_id'])['item_cnt_day'].cumsum()\n    test['cumulative_sales'] = 0  \n\n    # Time-Based Features\n    for df in [train, test]:\n        df['month'] = df['date'].dt.month\n        df['day_of_week'] = df['date'].dt.dayofweek\n        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n\n    # Lag Features (Only for train)\n    train = train.sort_values(by=['shop_id', 'item_id', 'date'])\n    train['lag_1'] = train.groupby(['shop_id', 'item_id'])['item_cnt_day'].shift(1)\n    train['lag_12'] = train.groupby(['shop_id', 'item_id'])['item_cnt_day'].shift(12)\n\n    # Forward-fill missing values for lags\n    train[['lag_1', 'lag_12']] = train[['lag_1', 'lag_12']].fillna(method='ffill')\n\n    # Use latest lag values from training data for test set\n    latest_lags = train.groupby(['shop_id', 'item_id'], as_index=False)[['lag_1', 'lag_12']].last()\n    test = test.merge(latest_lags, on=['shop_id', 'item_id'], how='left')\n\n    # Final missing value handling\n    train.fillna(0, inplace=True)\n    test.fillna(0, inplace=True)\n\n    # Ensure column order is consistent\n    test = test[train.columns.drop('item_cnt_day', errors='ignore')]  # Remove 'item_cnt_day' if it exists\n\n    return train, test\n\n# Apply feature creation to both train and test dataframes\ntrain, test = create_features(train, test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To get the monthly we need to aggregate by date_block_num, item_id, and shop_id\nitem_cnt_month = train.groupby(['date_block_num', 'item_id', 'shop_id'])['item_cnt_day'].sum().reset_index()\n\n# Rename column for clarity\nitem_cnt_month.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\n\n# Merge item_cnt_month back to the main dataframe\ntrain = train.merge(item_cnt_month, on=['date_block_num', 'item_id', 'shop_id'], how='left')\n\n# Now item_cnt_month contains the total sales per item per shop per month","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# align test and merged_df except the target column item_cnt_day/ item_cnt_month\n\n# Assuming you have 'train' and 'test' dataframes\n\n# Get the column names as sets\ntrain_columns = set(train.columns)\ntest_columns = set(test.columns)\n\n# Find missing columns in test\nmissing_in_test = train_columns - test_columns\n# Find missing columns in train\nmissing_in_train = test_columns - train_columns\n\n# Print the results\nprint(\"Columns missing in test:\")\nprint(missing_in_test)\n\nprint(\"\\nColumns missing in train:\")\nprint(missing_in_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\"\n# Copy Id for submission\ntest_id = test_df['ID'].copy()\n\n# Identify columns present in train_df but not in test_df \nmissing_columns = set(train_df.columns) - set(test_df.columns) - {'item_cnt_day'}\n\n# Add missing columns to test_df with default values 0\nfor col in missing_columns:\n    test_df[col] = 0  \n\n# Ensure both dataframes have the same column order \ncommon_columns = [col for col in train_df.columns if col in test_df.columns and col != 'item_cnt_day']\n\n# Reorder columns\ntrain_df = train_df[common_columns + ['item_cnt_day']]\ntest_df = test_df[common_columns]\n\n# Verify that the columns are aligned \nassert set(train_df.columns) == set(test_df.columns).union({'item_cnt_day'}), \"Columns do not match\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Label encode object dtype columns\nle = LabelEncoder()\nobject_cols = train.select_dtypes(include='object').columns\nfor col in object_cols:\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.fit_transform(test[col])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# modeling","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\n# Define the feature set and target variable\nX = train.drop(['item_cnt_day', 'item_cnt_month', 'date', 'item_name', 'item_category_name', 'shop_name'], axis=1)\ny = train['item_cnt_month']\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define parameter distribution for random search\nparam_dist = {\n    'num_leaves': randint(20, 3000),\n    'learning_rate': uniform(0.01, 0.3),\n    'feature_fraction': uniform(0.5, 0.5),\n    'bagging_fraction': uniform(0.5, 0.5),\n    'bagging_freq': randint(1, 7),\n    'min_child_samples': randint(10, 100),\n    'max_depth': randint(3, 12),\n    'n_estimators': randint(100, 1000)\n}\n\n# Initialize LightGBM regressor\nlgb_estimator = lgb.LGBMRegressor(objective=\"regression\", metric=\"rmse\", \n                                  random_state=42)\n\n# Perform Random Search with cross-validation\nrandom_search = RandomizedSearchCV(\n    estimator=lgb_estimator,\n    param_distributions=param_dist,\n    n_iter=100,  # Number of parameter settings that are sampled\n    cv=3,\n    scoring='neg_root_mean_squared_error',\n    verbose=-1,\n    n_jobs=-1,\n    random_state=42\n)\n\n# Fit the Random Search\nrandom_search.fit(X_train, y_train)\n\n# Best parameters from Random Search\nbest_params = random_search.best_params_\nprint(\"Best parameters found by Random Search:\", best_params)\n\n# Train the final model with the best parameters\nfinal_model = lgb.LGBMRegressor(**best_params, objective=\"regression\", \n                                metric=\"rmse\", random_state=42)\nfinal_model.fit(X_train, y_train)\n\n# Predict on the validation set\ny_pred = final_model.predict(X_val)\n\n# Evaluate the final model\nmse_final = mean_squared_error(y_val, y_pred, multioutput='raw_values')\nrmse_final = np.sqrt(mse_final)\nr2_final = r2_score(y_val, y_pred, multioutput='raw_values')\n\nprint(\"Final Model RMSE:\", rmse_final)\nprint(\"Final Model R-squared:\", r2_final)\n\n# Feature importance\nfeature_importance = final_model.feature_importances_\nfeature_names = X.columns\nfor importance, name in sorted(zip(feature_importance, feature_names), \n                               reverse=True):\n    print(f\"{name}: {importance}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# predict on test set","metadata":{}},{"cell_type":"code","source":"# Drop date\nX_test = test_df.drop(['date'], axis=1)\n\n# Check the test set has the same columns as the training set\nmissing_cols = set(X_train.columns) - set(X_test.columns)\nfor col in missing_cols:\n    X_test[col] = 0\nX_test = X_test[X_train.columns]\n\n# Predict on the test dataset\ny_test_pred = final_model.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# submission","metadata":{}},{"cell_type":"code","source":"# Save the predictions\npredictions_df = pd.DataFrame({'id': test_id, 'item_cnt_month': y_test_pred})\n\n# Save predictions to a CSV file\npredictions_df.to_csv('submission.csv', index=False)\n\n# Display the first few rows of the predictions\npredictions_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vanpatangan/predict-podcast-listening-time?scriptVersionId=233267661\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Import Libraries and Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read in data\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e4/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e4/test.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def check(df):\n    \"\"\"\n    Generates a concise summary of DataFrame columns.\n    \"\"\"\n    # Use list comprehension to iterate over each column\n    summary = [\n        [col, df[col].dtype, df[col].count(), df[col].nunique(), df[col].isnull().sum(), df.duplicated().sum()]\n        for col in df.columns\n    ]\n\n    # Create a DataFrame from the list of lists\n    df_check = pd.DataFrame(summary, columns=[\"column\", \"dtype\", \"instances\", \"unique\", \"sum_null\", \"duplicates\"])\n\n    return df_check","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training Data Summary\")\ndisplay(check(train))\ndisplay(train.head())\n\nprint(\"Test Data Summary\")\ndisplay(check(test))\ndisplay(test.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizations","metadata":{}},{"cell_type":"code","source":"# Distribution of Listening_Time_minutes\nsns.histplot(data=train, x='Listening_Time_minutes', bins=30, kde=True)\nplt.title('Distribution of Listening Time (Minutes)')\nplt.xlabel('Listening Time (minutes)')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Listening_Time_minutes across different Genre categories\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=train, x='Genre', y='Listening_Time_minutes')\nplt.xticks(rotation=45)\nplt.title('Listening Time by Genre')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Episode Length vs. Listening Time\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=train, x='Episode_Length_minutes', y='Listening_Time_minutes', alpha=0.5)\nplt.title('Episode Length vs. Listening Time')\nplt.xlabel('Episode Length (minutes)')\nplt.ylabel('Listening Time (minutes)')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation Matrix of Numerical Features\nplt.figure(figsize=(8, 6))\nsns.heatmap(train[['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Listening_Time_minutes']].corr(), annot=True, cmap='RdBu', vmin=-1, vmax=1)\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Listening Time by Episode Sentiment\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=train, x='Episode_Sentiment', y='Listening_Time_minutes')\nplt.title('Listening Time by Episode Sentiment')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Average Listening_Time_minutes for each Publication_Day\nplt.figure(figsize=(12, 6))\nsns.barplot(data=train, x='Publication_Day', y='Listening_Time_minutes', estimator='mean', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.title('Average Listening Time by Publication Day')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Define numerical columns with missing values\nnum_cols = ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads']\n\n# Imputer (fit on train, apply to both)\nimputer = SimpleImputer(strategy='median')\ntrain[num_cols] = imputer.fit_transform(train[num_cols])\ntest[num_cols] = imputer.transform(test[num_cols]) # Use train's median","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert Publication_Time to numerical bins \ntime_mapping = {\n    'Night': 0,\n    'Morning': 6,\n    'Afternoon': 12,\n    'Evening': 18\n}\n\ntrain['Publication_Hour'] = train['Publication_Time'].map(time_mapping)\ntest['Publication_Hour'] = test['Publication_Time'].map(time_mapping)\n\n# Drop original column\ntrain.drop('Publication_Time', axis=1, inplace=True)\ntest.drop('Publication_Time', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Interaction Features","metadata":{}},{"cell_type":"code","source":"# Episode Length / Popularity\ntrain['Length_Host_Popularity'] = train['Episode_Length_minutes'] * train['Host_Popularity_percentage']\ntest['Length_Host_Popularity'] = test['Episode_Length_minutes'] * test['Host_Popularity_percentage']\ntrain['Length_Guest_Popularity'] = train['Episode_Length_minutes'] * train['Guest_Popularity_percentage']\ntest['Length_Guest_Popularity'] = test['Episode_Length_minutes'] * test['Guest_Popularity_percentage']\n\n# Ads / Episode Length \ntrain['Ads_Length_Ratio'] = train['Number_of_Ads'] / (train['Episode_Length_minutes'] + 1e-6)  \ntest['Ads_Length_Ratio'] = test['Number_of_Ads'] / (test['Episode_Length_minutes'] + 1e-6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode categorical variables \n\n# Target encoding for high cardinality columns (Too many unique values for one hot encoding)\nfor col in ['Podcast_Name', 'Episode_Title']:\n    mean_target = train.groupby(col)['Listening_Time_minutes'].mean()\n    train[col + '_encoded'] = train[col].map(mean_target)\n    test[col + '_encoded'] = test[col].map(mean_target).fillna(mean_target.mean())  # Fill unseen categories\n\n# One hot encoding for low cardinality columns\ncat_cols = ['Genre', 'Publication_Day', 'Episode_Sentiment']\ntrain = pd.get_dummies(train, columns=cat_cols, drop_first=True)\ntest = pd.get_dummies(test, columns=cat_cols, drop_first=True)\n\n# Align train and test columns (in case of mismatched categories)\ntrain, test = train.align(test, join='left', axis=1, fill_value=0)\n\n# Drop unnecessary columns\ntest_id = test['id'].copy() # For submission later\ncolumns_to_drop = ['Podcast_Name','Episode_Title', 'id']\n\ntrain.drop(columns=columns_to_drop, inplace=True)\ntest.drop(columns=columns_to_drop, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Popularity Ratio / Sentiment\ntrain['Host_Popularity_Sentiment_Positive'] = train['Host_Popularity_percentage'] * train.get('Episode_Sentiment_Positive', 0)\ntest['Host_Popularity_Sentiment_Positive'] = test['Host_Popularity_percentage'] * test.get('Episode_Sentiment_Positive', 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train.drop(columns=['Listening_Time_minutes'])  # Features\ny = train['Listening_Time_minutes']  # Target\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Hyperparameters to tune\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'gbdt',\n        'num_leaves': trial.suggest_int('num_leaves', 20, 100),  # Range to search\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),  # Log scale for learning rate\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),  # Feature subsampling\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),  # Data subsampling\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),  # Frequency of bagging\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),  # Min data in leaf\n        'max_depth': trial.suggest_int('max_depth', 3, 10),  # Tree depth\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),  # L1 regularization\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),  # L2 regularization\n        'verbose': -1\n    }\n\n    # Initialize KFold\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    rmse_scores = []\n\n    # Cross-validation loop\n    for train_idx, val_idx in kf.split(X):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # Create LightGBM datasets\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n        # Train the model\n        model = lgb.train(\n            params,\n            train_data,\n            num_boost_round=1000,\n            valid_sets=[val_data],\n            valid_names=['validation'],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50),\n                lgb.log_evaluation(period=0)  # Silent output during tuning\n            ]\n        )\n\n        # Predict and calculate RMSE\n        y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n        rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n        rmse_scores.append(rmse)\n\n    # Return the average RMSE across folds\n    return np.mean(rmse_scores)\n\n# Create and run the Optuna study\nstudy = optuna.create_study(direction='minimize')  # Minimize RMSE\nstudy.optimize(objective, n_trials=50)  # Run 50 trials (adjust as needed)\n\n# Print the results\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(f\"  RMSE: {trial.value:.4f}\")\nprint(\"  Best hyperparameters:\", trial.params)\n\n# Train the final model with the best parameters\nbest_params = trial.params\nbest_params.update({\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'verbose': -1\n})\n\ntrain_data = lgb.Dataset(X, label=y)  # Use full dataset for final model\nfinal_model = lgb.train(\n    best_params,\n    train_data,\n    num_boost_round=1000,\n    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n)\n\nprint(\"Final model trained with optimized parameters!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n# Initialize KFold for crossvalidation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmse_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n    # Split data for this fold\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # Create LightGBM datasets\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n    # Define parameters\nparams = {\n    'objective': 'regression',          # For predicting target var\n    'metric': 'rmse',                   # Root Mean Squared Error\n    'boosting_type': 'gbdt',            # Gradient Boosted Decision Trees\n    'num_leaves': 74,                   # Reduced to prevent overfitting\n    'learning_rate': 0.09,              # Smaller = slower, more robust\n    'feature_fraction': 0.9,            # Feature subsampling\n    'bagging_freq': 7,\n    'min_child_samples': 93,\n    'max_depth': 8,\n    'verbose': -1                       # Suppress warnings\n}\n\n# Train with early stopping using callback\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=2000,               # Max iterations\n    valid_sets=[val_data],              # Validation set for monitoring\n    valid_names=['validation'],         # Name for validation set in output\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=100), # Stop if no improvement for 100 rounds\n        lgb.log_evaluation(period=10)            # Print progress every 10 rounds\n    ]\n)\n\n # Predict and calculate RMSE\ny_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\nrmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\nrmse_scores.append(rmse)\n    \nprint(f\"Fold {fold} RMSE: {rmse:.4f}\")\nprint(f\"Fold {fold} Best Iteration: {model.best_iteration}\\n\")\n\n # Cross validated performance\nprint(f\"Average Validation RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb.plot_importance(model, max_num_features=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Prediction","metadata":{}},{"cell_type":"code","source":"# Align columns\nX_val = X_val[X_train.columns]\ntest = test[X_train.columns]\n\n# Predict on test set\ntest_pred = model.predict(test, num_iteration=model.best_iteration)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Create submission\nsubmission = pd.DataFrame({'id': test_id, 'Listening_Time_minutes': test_pred})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
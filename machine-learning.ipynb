{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Machine Learning: Key Points**\n\n---\n\n### **Module 1: Introduction to Machine Learning**\n- **What is Machine Learning?**\n  - Answer: Machine Learning is a subset of AI where computers learn from data without explicit programming.\n  \n- **Types of Machine Learning**\n  - **Supervised Learning**: Training on labeled data (Example: Classification, Regression).\n  - **Unsupervised Learning**: Learning from unlabeled data (Example: Clustering, Dimensionality Reduction).\n  - **Reinforcement Learning**: Learning by interacting with an environment to maximize a reward.\n\n- **Key Concepts**\n  - **Model**: A mathematical representation of a real-world process.\n  - **Feature**: An input variable used to make predictions.\n  - **Label/Target**: The output the model predicts.\n\n---\n\n### **Module 2: Data Preprocessing and Feature Engineering**\n- **Why Preprocessing?**\n  - Answer: Data needs to be clean and in a suitable format for models to learn effectively.\n  \n- **Steps in Data Preprocessing**\n  - Handling Missing Values: Imputation (Mean/Median/Mode, Forward Fill).\n  - Encoding Categorical Data: Label Encoding, One-Hot Encoding.\n  - Feature Scaling: Normalization (Min-Max Scaling), Standardization (Z-score scaling).\n  \n- **Feature Engineering**\n  - Answer: Creating new features from existing ones to improve model performance.\n  - Examples: Binning, interaction terms, polynomial features.\n\n---\n\n### **Module 3: Supervised Learning Algorithms**\n- **Linear Regression**\n  - Answer: Predicts a continuous value by fitting a line through data points.\n  \n- **Logistic Regression**\n  - Answer: A classification algorithm for binary outcomes (Yes/No, 1/0).\n  \n- **Decision Trees and Random Forest**\n  - Answer: A tree-based model that splits data based on feature values. Random Forest is an ensemble of decision trees.\n  \n- **Support Vector Machines (SVM)**\n  - Answer: Finds the hyperplane that best separates data into classes.\n\n---\n\n### **Module 4: Unsupervised Learning Algorithms**\n- **K-Means Clustering**\n  - Answer: A clustering algorithm that partitions data into K groups.\n  \n- **Principal Component Analysis (PCA)**\n  - Answer: Reduces the dimensionality of data while preserving as much variance as possible.\n  \n- **Hierarchical Clustering**\n  - Answer: Builds a tree-like structure (dendrogram) to group data points.\n\n---\n\n### **Module 5: Model Evaluation and Tuning**\n- **Evaluation Metrics**\n  - **Regression**: Mean Absolute Error (MAE), Mean Squared Error (MSE), RÂ² Score.\n  - **Classification**: Accuracy, Precision, Recall, F1 Score, AUC-ROC.\n  \n- **Train-Test Split**\n  - Answer: Split data into training and testing sets to evaluate model performance.\n  \n- **Cross-Validation**\n  - Answer: A technique to assess model performance using different subsets of data.\n  \n- **Hyperparameter Tuning**\n  - Answer: Finding the best settings (parameters) for the algorithm (Example: Grid Search, Random Search).\n\n---\n\n### **Module 6: Advanced Topics**\n- **Ensemble Methods**\n  - **Bagging**: Combining models to reduce variance (Example: Random Forest).\n  - **Boosting**: Combining models to reduce bias (Example: Gradient Boosting, XGBoost).\n  \n- **Neural Networks and Deep Learning**\n  - Answer: Algorithms modeled after the human brain that can learn from vast amounts of data.\n  \n- **Natural Language Processing (NLP)**\n  - Answer: Machine Learning techniques applied to text (Example: Sentiment Analysis, Text Classification).\n\n---\n\n### **Module 7: Real-World Applications**\n- **Use Cases**\n  - Healthcare: Disease prediction (Example: Predicting readmissions).\n  - Finance: Fraud detection (Example: Predicting fraudulent transactions).\n  - Retail: Customer segmentation, demand forecasting.\n  \n- **Deployment of Machine Learning Models**\n  - Answer: How to integrate trained models into production systems (Example: APIs, cloud deployment).\n\n---\n\n### **Module 8: Ethics and Challenges in Machine Learning**\n- **Bias and Fairness in ML**\n  - Answer: Avoiding biased models and ensuring fairness in predictions.\n  \n- **Interpretability and Explainability**\n  - Answer: Understanding how models make predictions (Example: SHAP values, LIME).\n\n- **Overfitting and Underfitting**\n  - Answer: Overfitting happens when the model learns noise, underfitting when it doesn't capture patterns.\n\n","metadata":{}}]}